from fastapi import Form
from main import app, create_embedding, cosine_similarity, client, DEFAULT_MODEL
from firebase_admin import firestore


@app.post("/ask-with-embeddings/")
async def ask_with_embeddings(
    question: str = Form(...),
    file_id: str = Form(...),
    user_id: str = Form(...),
    chat_id: str = Form(...)
):
    from firebase_admin import firestore
    db = firestore.client()

    print("\n[ask-with-embeddings] ğŸ“¥ Ä°stek alÄ±ndÄ±")
    print(f"   â†’ user_id: {user_id}")
    print(f"   â†’ chat_id: {chat_id}")
    print(f"   â†’ file_id: {file_id}")
    print(f"   â†’ question: {question}")

    # Firestore Path
    base_ref = db.collection("embeddings").document(user_id).collection(chat_id)
    print(f"[ask-with-embeddings] ğŸ”— Firestore path: embeddings/{user_id}/{chat_id}")

    # 1. Soru embedding oluÅŸtur
    try:
        q_embedding = create_embedding(question)
        print(f"[ask-with-embeddings] ğŸ§  Soru embedding boyutu: {len(q_embedding)}")
    except Exception as e:
        print(f"[ask-with-embeddings] âŒ Soru embedding hatasÄ±: {e}")
        raise

    # 2. Ä°lgili chunk'larÄ± Ã§ek (meta hariÃ§)
    try:
        docs = base_ref.where("file_id", "==", file_id).stream()
        print("[ask-with-embeddings] ğŸ”„ Firestore'dan chunk'lar Ã§ekiliyor...")

        chunks = []
        for doc in docs:
            data = doc.to_dict()
            if data.get("chunk_index", 0) >= 0:  # Pythonâ€™da filtre
                score = cosine_similarity(q_embedding, data["embedding"])
                chunks.append((score, data["text"]))
                print(f"   â†’ Chunk skor: {score:.4f}, text (ilk 60): {data['text'][:60]}")
    except Exception as e:
        print(f"[ask-with-embeddings] âŒ Chunk okuma hatasÄ±: {e}")
        raise

    # 3. En yakÄ±n 3 chunk seÃ§
    top_chunks = [text for score, text in sorted(chunks, key=lambda x: x[0], reverse=True)[:3]]
    print(f"[ask-with-embeddings] ğŸ† SeÃ§ilen top_chunks (adet: {len(top_chunks)}):")
    for i, tc in enumerate(top_chunks):
        print(f"   {i+1}. {tc[:100]}...")

    # 4. GPT prompt hazÄ±rla
    context = "\n".join(top_chunks)
    prompt = f"""\nBaÄŸlama dayanarak soruya yanÄ±t ver. DoÄŸrudan cevap yoksa en yakÄ±n bilgiyi Ã¶zetle:\n{context}\n\nSoru: {question}\n"""
    print(f"[ask-with-embeddings] ğŸ“ GPT'ye gÃ¶nderilen prompt (ilk 300): {prompt[:300]}")

    # 5. GPT Ã§aÄŸÄ±r ve cevap dÃ¶ndÃ¼r
    try:
        response = client.chat.completions.create(
            model=DEFAULT_MODEL,
            messages=[
                {"role": "system", "content": "Sen baÄŸlam tabanlÄ± bir asistansÄ±n."},
                {"role": "user", "content": prompt}
            ]
        )
        answer = response.choices[0].message.content
        print(f"[ask-with-embeddings] âœ… GPT yanÄ±tÄ± (ilk 200): {answer[:200]}")
    except Exception as e:
        print(f"[ask-with-embeddings] âŒ GPT yanÄ±t hatasÄ±: {e}")
        raise

    return {"answer": answer, "context_used": top_chunks}
